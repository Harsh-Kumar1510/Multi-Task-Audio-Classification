{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda, no_grad\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import  Dict\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model:nn.Module, test_data:DataLoader, device:str)->Dict:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be evaluated.\n",
    "        test_data (DataLoader): The PyTorch DataLoader containing the test data.\n",
    "        device (str): The device to be used for the computation (e.g., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the following key-value pairs:\n",
    "            - 'accuracy_digit': Accuracy of the digit classification task as a percentage.\n",
    "            - 'accuracy_gender': Accuracy of the gender classification task as a percentage.\n",
    "            - 'digit_predict': A numpy array containing the predicted digit labels.\n",
    "            - 'digit_gt': A numpy array containing the ground truth digit labels.\n",
    "            - 'gen_predict': A numpy array containing the predicted gender labels.\n",
    "            - 'gen_gt': A numpy array containing the ground truth gender labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Keep tract correct prediction\n",
    "    correct_digit = 0\n",
    "    correct_gen = 0\n",
    "    total = 0\n",
    "\n",
    "    # Collect prediction for confusion matrix\n",
    "    digit_predict = []\n",
    "    digit_gt =[]\n",
    "    gen_predict = []\n",
    "    gen_gt = []\n",
    "\n",
    "    # Disable gradient calculation \n",
    "    with torch.no_grad():\n",
    "        for batch in test_data:\n",
    "            # Get features and labels\n",
    "            test_feat = batch['feature'].float()\n",
    "            digit_label = batch['digit_label']\n",
    "            digit_label = torch.argmax(digit_label, dim=1).long() # label indices \n",
    "            gen_label = batch['gen_label'].float()\n",
    "\n",
    "            # Give them to the appropriate device\n",
    "            test_feat = test_feat.to(device)\n",
    "            digit_label = digit_label.to(device)\n",
    "            gen_label = gen_label.to(device)\n",
    "            \n",
    "            # Make the prediction\n",
    "            gen_pred, digit_pred = model(test_feat)\n",
    "            \n",
    "            # Gender prediction: above 0.5 th male, otherwise female class\n",
    "            gen_pred = (F.sigmoid(gen_pred) > 0.5).int().squeeze(dim=1) \n",
    "            gen_label = gen_label.int().squeeze(dim=1)\n",
    "            gen_predict.extend(gen_pred.cpu().numpy())\n",
    "            gen_gt.extend(gen_label.cpu().numpy())\n",
    "            \n",
    "            # Digit prediction class indices\n",
    "            digit_pred = digit_pred.argmax(dim=1) \n",
    "            digit_predict.extend(digit_pred.cpu().numpy())\n",
    "            digit_gt.extend(digit_label.cpu().numpy())\n",
    "            \n",
    "            # Keep track  correct prediction\n",
    "            total += digit_label.shape[0] # total samples\n",
    "            correct_digit += (digit_pred == digit_label).sum().item() # digit\n",
    "            correct_gen += (gen_pred == gen_label).sum().item() # gender\n",
    "    \n",
    "    accuracy_digit = 100 * correct_digit / total\n",
    "    accuracy_gen = 100 * correct_gen / total\n",
    "    print(f'Accuracy on digit classificaiton: {accuracy_digit: .3f}')\n",
    "    print(f'Accuracy on gender classificaiton: {accuracy_gen: .3f}')\n",
    "\n",
    "    return {\"accuracy_digit\": accuracy_digit,\n",
    "            \"accuracy_gender\": accuracy_gen,\n",
    "            \"digit_predict\": np.array(digit_predict), \n",
    "            \"digit_gt\": np.array(digit_gt), \n",
    "            \"gen_predict\": np.array(gen_predict),\n",
    "            \"gen_gt\": np.array(gen_gt)}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
